import itertools as it
import pickle
import math, random

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import brute
import scipy.special

import warnings
warnings.filterwarnings("error")

#DO_TRAINING = False
#DO_ANALYSIS = True


####################
# HELPER FUNCTIONS #
####################


def off_patterns(pat, p_off, num_pats):
    """
    Returns num_pat patterns, each generated by duplicating pat
    and randomly flipping each value with probability p_off.

    Args:
        pat (np.array): Binary vector of length d. Each element is -1 or +1
        p_off (float in [0,1]): Probability of each node being corrupted.
        num_pats (int): Number of corrupted patterns to return.

    Returns:
        len(pat) x num_pats 2darray (dtype=float) with corrupted patterns as columns.
    """
    d = len(pat)

    corrupt_mask = np.random.default_rng().random((num_pats, d))
    out = np.tile(pat, (num_pats,1))

    out[corrupt_mask < p_off] = -out[corrupt_mask < p_off]
    return [out[i,:] for i in range(num_pats)]

##################
# LOSS FUNCTIONS #
##################

def rmse(pat, pred):
    """
    Root Mean Square Error function.
    """
    return np.mean((pat - pred) ** 2) ** 0.5

## TODO: does this still work for dual-rail, with the output determined by fluorescence rather than probability?
def drmse(pat, pred):
    """
    Analytical gradient of RMSE, elementwise.
    """
    return ( 1/rmse(pat, pred) * 1/len(pat) * (pred - pat) ).T

########################
# GRADIENT CALCULATION #
########################

def calc_network_output_sr(rate_matrix, input_rates, output_rates):
    """
    Computes the single-rail network output given all rate parameters (k_in, k_out, k_ij).
    The single-rail network output is the output fluorescence from each node, given by p_i * k^i_out.

    Args:
        rate_matrix (np.array): The weights (rate constants, arbitrary units) between nodes in the system.
            Should be square and symmetric, with all diagonal entries equal to 0.
        input_rates (np.array): The intrinsic excitation rates of each node (k_in).
            Should be length-n 1d array, for network of n nodes.
        output_rates (np.array): The intrinsic emission rate constants of each node (k_out). 
            Should be a length-n 1d array, for network of n nodes.
    
    Returns:
        Ainv (np.array): The inverse of the matrix representing the linear system of equations.
            Should be square.
        pred (np.array): The predicted values of each node's output.
    """
    num_nodes = len(input_rates)

    if rate_matrix[range(num_nodes), range(num_nodes)].any():
        raise ValueError(f'diagonal terms not 0 in the rate matrix: \n {rate_matrix}')

    A = -rate_matrix
    diagonal_terms = rate_matrix.sum(axis=1) + input_rates.T + output_rates.T
    A[range(num_nodes), range(num_nodes)] = diagonal_terms

    try:
        Ainv = np.linalg.inv(A)
    except:
        raise ValueError(f'Singular matrix for rates={rate_matrix}, inputs={input_rates}, outputs={output_rates}')

    pred = Ainv @ input_rates
#    print(pred)
#    print(output_rates)
    return Ainv, pred * output_rates

def calc_network_output_dr(input_pattern, rate_matrix_sr, output_rates_sr):
    """
    Computes the dual-rail network output given an input pattern and parameters of the single-rail system
    (k_ij and k_out). The intrinsic excitation rate constants for the single-rail system are derived
    from the input pattern. The number of nodes in the dual-rail system (n) should be half the number
    in the single-rail system (2n).

    It is assumed that node i in the dual-rail network has nodes 2i and 2i+1 from the single-rail network
    as its + and - fluorophores, respectively.

    Args:
        input_pattern (np.array): The binary input data. 
            Should be length-n 1d array with each element equal to -1 or +1.
        rate_matrix_sr (np.array): A matrix of the FRET rate constants between nodes in the single-rail system. 
            Should be a square and symmetric 2nx2n matrix, with diagonal entries equal to 0.
        output_rates_sr (np.array): The intrinsic emission rate constants of each node in the single-rail system (k_out).
            Should be a length-2n 1d nonnegative array

    Returns:
        output_dr (np.array): The dual-rail outputs, defined as the difference between the output fluorescence from
            each dual-rail node's + and - fluorophores in the single-rail network. Should be length-n 1d array.
        output_sr (np.array): The single-rail outputs, defined as the output fluorescence from each single-rail node.
            Should be a length-2n 1d array.
        Ainv (np.array): The inverse A matrix computed during calculation of the single-rail output. Returned as an
            optimization because it is used for other operations. Should be square and symmetric.
        
    """
    num_nodes_dr = len(input_pattern)
    num_nodes_sr = 2*num_nodes_dr

    # Determine equivalent single-rail input rates based on dual-rail input pattern
    input_rates_sr = np.array([
        kin for bit in input_pattern for kin in (max(bit, 0), -min(bit, 0))
    ])

    Ainv, output_sr = calc_network_output_sr(rate_matrix_sr, input_rates_sr, output_rates_sr)

    output_dr = output_sr[range(0, num_nodes_sr, 2)] - output_sr[range(1, num_nodes_sr, 2)]
#    print(output_sr)
#    print(output_dr)

    return output_dr, output_sr, Ainv
    

# note: not yet modified for use with dual-rail
def gradient(loss_grad, pat, pred, Ainv, output_rates, verbose=False):
    """
    Finds the gradient of a loss function with respect to rates in a FRETnet.

    Args:
        loss_grad: The gradient of the loss function with respect to pred.
            Should take pat and pred as parameters and return a array of same size.
        pat (np.array): The given training pattern.
            Should be a dx1 binary column vector.
        pred (np.array): The predicted output of each node.
            Should be a dx1 column of probabilities.
        Ainv (np.array): The inverse of the matrix representing the linear system.
            Should be a nxn square matrix. 
        output_rates (np.array): The intrinsic output rate constants assigned to each node.
            Should be a dx1 column vector.
        verbose (bool): If True, returns a dict with keys:
            Ainv, pred, dL_dpred, dpred_dAinv, dAinv_dA, dA_dK, dL_dK

    Returns: 
        A dict of all the quantities computed, if verbose is True.
        dL_dK (np.array): The gradient of the loss with respect to the weights of the network.
            Should be a dxd matrix, reshaped from 1xd^2.

    """
    # don't actually have to compute the NLL, just its gradient
    num_nodes = len(pat)

    dL_dpred = loss_grad(pat, pred) # 1 x d

    dpred_dAinv = np.kron(pat, np.identity(num_nodes)).T # d x d^2
    
    dAinv_dA = -np.kron(Ainv, Ainv) # d^2 x d^2

    dA_dK = np.zeros((num_nodes**2, scipy.special.binom(num_nodes, 2))) # d^2 x (d choose 2)
    idx = 0
    for i,j in it.combinations(range(num_nodes), 2):
        dA_dK[i*num_nodes + i, idx] = 1
        dA_dK[j*num_nodes + j, idx] = 1
        dA_dK[i*num_nodes + j, idx] = -1
        dA_dK[j*num_nodes + i, idx] = -1
        idx += 1

    dL_dK = (dL_dpred @ dpred_dAinv @ dAinv_dA @ dA_dK).flatten()

    reshaped_dL_dK = np.zeros((num_nodes, num_nodes))
    idx = 0
    for i,j in it.combinations(range(num_nodes), 2):
        reshaped_dL_dK[i,j] = dL_dK[idx]
        idx += 1
    reshaped_dL_dK = reshaped_dL_dK + reshaped_dL_dK.T
    # 1 x (d choose 2) -> d x d, duplicating items for symmetry
    # and making diagonal terms 0

    if verbose:
        return {'Ainv':Ainv, 'pred':pred, 'dL_dpred':dL_dpred, 'dpred_dAinv':dpred_dAinv, 
            'dAinv_dA':dAinv_dA, 'dA_dK':dA_dK, 'reshaped_dL_dK':reshaped_dL_dK}
    else:
        return reshaped_dL_dK
    

############
# TRAINING #
############
    
#def train(train_data, loss_fn, loss_grad, output_rates, step_size, iters, epsilon = None, noise = 0.1, num_corrupted=1, report_every=0):
#    """
#    Trains a new network on given training patterns using batch gradient descent.
#    Patterns with specified amount of noise are used as input to a simulated FRETnet, 
#    then steady-state behavior is compared to original patterns using a loss function.
#    Stops either when gradient step dK is less than epsilon or after iters iterations.
#
#    Args:
#        train_data (np.array): 2d float array where each column is a training pattern.
#            Should be dxn, where d is number of nodes and n is number of patterns.
#        ouput_rates (np.array): The intrinsic outputs of each node.
#            Should be a dx1 column vector. 
#        step_size (float): Multiplier for each gradient descent update.
#        iters (int): Number of times to pass through the training data.
#        epsilon (float): Size of gradient at which training should stop.
#        noise (float in [0,1]): Extend of deviation of training patterns 
#            from the given train_data.
#        num_corrupted (int): Num of corrupted patterns to generate 
#            from each template pattern. 
#        report_every (int): If True, prints all rate arrays while training.
#
#    Returns:
#        weights (np.array): Weights between nodes in the network. 
#            Should be dxd, NONNEGATIVE and symmetric.
#        err_over_time (list): Average error over training patterns,
#            recorded for each iteration loop.
#            Should have length n.
#    """
#    d, n = train_data.shape
#    K = np.random.rand(d, d) / 10 + 0.45  # weights initialized at 0.5 +/- 0.05
#    
#    K = (K + K.T) / 2  # ensure starting weights are symmetric
#    np.fill_diagonal(K, 0)
#    err_over_time = []
#    K_over_time = np.array(K).reshape((1, d, d))
#
#    for i in range(iters):
#        avg_err = 0
#        dK = 0
#
#        for j in range(n):  
#            template = train_data[:, j:j+1]
#            train_patterns = off_patterns(template, noise, num_corrupted)
#
#            for k in range(num_corrupted):
#                pat = train_patterns[:, k:k+1].astype(float) # ensure float
#
#                # TODO Find a better way to deal with singular matrices
#                #       arising from all-zero patterns
#                
#                pat[pat==0] = 0.1
#
#                # Compute the prediction based on the off patterns,
#                # But evaluate error relative to original template pattern.
#                Ainv, pred = _forward_pass(K, pat, output_rates)
#                # print(f'pat: \n{pat} \n pred: \n{pred}')
#                dL_dK = gradient(loss_grad, template, pred, Ainv, output_rates)
#                dK += dL_dK
#                avg_err += loss_fn(template, pred)/(n * num_corrupted)
#        
#        new_K = K.copy()
#        new_K -= step_size * dK
#        new_K[new_K<0] = 0 # NOTE ReLU; ensures all weights are positive.
#        err_over_time.append(avg_err)
#        K_over_time = np.append(K_over_time, new_K.reshape(1, d, d), axis=0)
#
#        # Stopping condition based on epsilon
#        if epsilon and np.linalg.norm(new_K - K) < epsilon:
#            print(f'Stopped at iteration {i+1}\n'
#                    f'K: {new_K}\n'
#                    f'error: {avg_err}\n'
#                    f'< epsilon: {np.linalg.norm(new_K - K)}')
#            return new_K, err_over_time, K_over_time
#    
#        if (i) % report_every == 0:
#            print(f'Rates on iteration {i}: \n{new_K}')
#
#        K = new_K
#
#    return K, err_over_time, K_over_time

def train_dr(stored_data, loss_func, duplication = 5, noise = 0.1, seed = None):
    def params_to_rates(p):
        K_fret = np.zeros((num_nodes_sr, num_nodes_sr))
        for idx, (i,j) in enumerate(it.combinations(range(num_nodes_sr),2)):
            K_fret[i,j] = p[idx]
            K_fret[j,i] = p[idx]
#        k_out = 10*np.ones((num_nodes_sr, 1)) # use this line for fixed, uniform k_out
#        k_out = p[-1]*np.ones((num_nodes_sr, 1)) # use this line for optimized, uniform k_out
        k_out = p[-num_nodes_sr:] # use this line for optimized, non-uniform k_out
        return K_fret, k_out

    def loss_func_scipy(params):
        K_fret, k_out = params_to_rates(params)

#        resid = np.empty((num_nodes_dr, len(train_data)))
#        for i, (input_data, output_data_cor) in enumerate(train_data):
#            output_data,_,_ = calc_network_output_dr(input_data, K_fret, k_out)
#            resid[:,i] = (output_data - output_data_cor).flatten()
#
#        return resid.flatten()

        resid = np.array([
            loss_func(calc_network_output_dr(input_data,K_fret,k_out)[0],output_data_cor) 
            for input_data,output_data_cor in train_data
        ])

        return resid

    num_nodes_dr = len(stored_data[0])
    num_nodes_sr = 2*num_nodes_dr
    train_data = [
        (input_data, output_data)
            for output_data in stored_data 
            for input_data in off_patterns(output_data, noise, duplication)
    ]

    num_params = num_nodes_sr*(num_nodes_sr-1)//2 + num_nodes_sr
    init_params = np.random.uniform(1, 10, num_params)
#    init_params = np.ones(num_params)
    res = scipy.optimize.least_squares(loss_func_scipy, init_params, bounds=(0,10**5), jac='3-point')

    return (*params_to_rates(res.x), res)
    

# NOTE: following code not modified for dual-rail
#def grid_search(num_nodes, pat, outs, loss_fn, k_domain, resolution=10, noise=0.1):
#    """
#    Grid-searches FRET rates with given intrinsic output rates, noise amt for the configuration with the lowest loss_fn.
#    """
#    def f(params):
#        K = np.reshape(params, (num_nodes, num_nodes))
#        _, pred = _forward_pass(K, pat, outs)
#        num_corrupted = 10
#        off_pats = off_patterns(pat, noise, num_corrupted)
#        avg_loss = 0
#        for i in range(num_corrupted):
#            off_pat = off_pats[:, i:i+1]
#            avg_loss += loss_fn(off_pat, pred) / num_corrupted
#        #TODO handle all zeros in off_pat
#        return avg_loss
#    
#    k_ranges = [tuple(k_domain) for _ in range(num_nodes**2)]
#    diag_indices = [(d*num_nodes + d) for d in range(num_nodes)]
#    for i in diag_indices:
#        k_ranges[i] = (0, 0)
#    print(k_ranges)
#
#    resbrute = brute(f, k_ranges, Ns=resolution, finish=None)
#    K_min = np.reshape(resbrute, (num_nodes, num_nodes))
#    return K_min

if __name__ == '__main__':

    num_nodes = 4
    num_patterns = 2
    stored_data_ints = random.sample(range(2**num_nodes), num_patterns)
    stored_data = [
        np.array([int(v)*2-1 for v in format(i,'0{}b'.format(num_nodes))])
        for i in stored_data_ints
    ]
#    stored_data = list(map(np.array, [[-1,-1,1],[1,-1,-1]]))


    print('Training data:', stored_data)

    trained_K_fret, trained_k_out, res = train_dr(stored_data, rmse, duplication=5)

    # this output is probably too much for bigger networks
    for i in range(2*num_nodes):
        print(trained_k_out[i], np.round(trained_K_fret[i,:],3).tolist())
    for d in map(np.array, it.product(*([[-1,1]]*num_nodes))):
        pre = '*' if list(d) in map(list, stored_data) else ' '
        print(pre, d.flatten(), calc_network_output_dr(d, trained_K_fret, trained_k_out)[0].flatten())

    output = {
      'num_nodes': num_nodes,
      'num_patterns': num_patterns,
      'stored_data': stored_data,
      'trained_K_fret': trained_K_fret,
      'trained_k_out': trained_k_out,
      'scipy_output': res
    }

    with open('output_nodes={}_pat={}.p'.format(num_nodes, num_patterns),'wb') as outfile:
      pickle.dump(output, outfile)
